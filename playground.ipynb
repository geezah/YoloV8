{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model.eval()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lengths(img : torch.Tensor):\n",
    "        h, w = img.size(2), img.size(3)\n",
    "        longest_side = torch.max(torch.tensor([h, w], dtype=torch.short).detach())\n",
    "        resize_value = torch.ceil(longest_side / 32) * 32\n",
    "        return h, w, resize_value.int().item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(img):\n",
    "        img = (img if isinstance(img, torch.Tensor) else torch.from_numpy(img)).to('cpu')\n",
    "        img = img.permute(0,3,1,2)\n",
    "        img = img.float()  # uint8 to fp16/32\n",
    "        h, w, resize_value = Wrapper.get_lengths(img)\n",
    "        padding = torch.zeros((1, 3, resize_value, resize_value))\n",
    "        padding[:, :, :h, :w] = img\n",
    "        padding /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        return padding\n",
    "    \n",
    "    @staticmethod\n",
    "    def xywh2xyxy(x):\n",
    "        y = x.clone()\n",
    "        y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "        y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "        y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "        y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def _non_max_suppression(pred, orig_img, conf_threshold=0.5, iou_threshold=0.4, max_det=300):\n",
    "        pred.squeeze_()\n",
    "        boxes, scores, cls = pred[:4, :].T, pred[4:, :].amax(0), pred[4:, :].argmax(0).to(torch.int)\n",
    "        keep = scores.argsort(0, descending=True)[:max_det]\n",
    "        boxes, scores, cls = boxes[keep], scores[keep], cls[keep]\n",
    "        boxes = Wrapper.xywh2xyxy(boxes)\n",
    "        candidate_idx = torch.arange(0, scores.shape[0])\n",
    "        candidate_idx = candidate_idx[scores > conf_threshold]\n",
    "\n",
    "        boxes, scores, cls = boxes[candidate_idx], scores[candidate_idx], cls[candidate_idx]\n",
    "        final_idx = torchvision.ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
    "\n",
    "        boxes = boxes[final_idx]\n",
    "        scores = scores[final_idx]\n",
    "        cls = cls[final_idx]\n",
    "\n",
    "        boxes[:, [0,2]] = boxes[:, [0,2]].clamp(min=0, max=orig_img.size(2)) # width for x \n",
    "        boxes[:, [1,3]] = boxes[:, [1,3]].clamp(min=0, max=orig_img.size(1)) # height for y\n",
    "                \n",
    "        return torch.cat([boxes, scores.unsqueeze(1), cls.unsqueeze(1)], dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def postprocess(pred, orig_img):\n",
    "        result = Wrapper._non_max_suppression(pred, orig_img)\n",
    "        return result\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        orig_img = imgs.clone()\n",
    "        imgs = Wrapper.preprocess(imgs)\n",
    "        preds = self.model(imgs)\n",
    "        result = Wrapper.postprocess(preds[0], orig_img)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/samples/image1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from torchvision.io import read_image\n",
    "\n",
    "image = read_image(path, mode=torchvision.io.ImageReadMode.RGB).unsqueeze(0).permute(0,2,3,1)\n",
    "yolo = YOLO(\"yolov8n.pt\", task='detect')\n",
    "model = yolo.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped = Wrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30008721351623535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.2067e+02, 3.9544e+02, 3.4574e+02, 8.6122e+02, 8.5562e-01, 0.0000e+00],\n",
       "        [6.6942e+02, 4.0530e+02, 8.0967e+02, 8.7953e+02, 8.4762e-01, 0.0000e+00],\n",
       "        [0.0000e+00, 2.1678e+02, 8.1000e+02, 7.5988e+02, 7.9169e-01, 5.0000e+00],\n",
       "        [5.4363e+01, 4.0058e+02, 2.0823e+02, 8.9836e+02, 7.8034e-01, 0.0000e+00],\n",
       "        [0.0000e+00, 5.5170e+02, 6.6415e+01, 8.7339e+02, 5.7889e-01, 0.0000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "result = wrapped(image)\n",
    "\n",
    "print(time() - start)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_245/1933337009.py:12: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  longest_side = torch.max(torch.tensor([h, w], dtype=torch.short).detach())\n",
      "/tmp/ipykernel_245/1933337009.py:14: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return h, w, resize_value.int().item()\n",
      "/home/emre/workspaces/repositories/YoloV8/.venv/lib/python3.10/site-packages/ultralytics/nn/modules.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif self.dynamic or self.shape != shape:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dynamic = {}\n",
    "dynamic['image'] = {1 : 'height', 2 : 'width'} # Input shape: (1, H, W, 3)\n",
    "dynamic['output'] = {0 : 'num_boxes'} # Output shape: (N, 6)\n",
    "\n",
    "torch.onnx.export(\n",
    "    wrapped, \n",
    "    image, \n",
    "    'wrapped_model.onnx',\n",
    "    input_names=['image'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes=dynamic if dynamic else None,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession('wrapped_model.onnx')\n",
    "inputs = sess.get_inputs()\n",
    "outputs = sess.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/samples/image2.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[     132.98,      41.823,      671.31,      684.01,     0.73314,           0],\n",
       "        [     52.443,       730.7,       158.1,         840,     0.50371,           0]], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from time import time\n",
    "\n",
    "image = read_image(path, mode=torchvision.io.ImageReadMode.RGB).unsqueeze(0)\n",
    "result = sess.run(\n",
    "    None,\n",
    "    {inputs[0].name : image.permute(0,2,3,1).numpy()}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', 'person']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_yolo_labels_for_image(result):\n",
    "    indices = list(result[0][:, 5].astype('int'))\n",
    "    labels = []\n",
    "    for k in indices:\n",
    "        labels.append(yolo.names[k])\n",
    "    return labels\n",
    "labels = get_yolo_labels_for_image(result)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotted = draw_bounding_boxes(image.squeeze(), torch.from_numpy(result[0][:, :4]), width=4, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import imageio.v3 as iio\n",
    "\n",
    "iio.imwrite(f'{Path(path).name}', plotted.permute(1,2,0).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39d91cea05d212b12221b5d2207b03cc88c9ad72bfc0ff987624f799986c9e15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
