{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model.eval()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lengths(img : torch.Tensor):\n",
    "        h, w = img.size(2), img.size(3)\n",
    "        longest_side = torch.max(torch.tensor([h, w], dtype=torch.short).detach())\n",
    "        resize_value = torch.ceil(longest_side / 32) * 32\n",
    "        return h, w, resize_value.int().item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(img):\n",
    "        img = (img if isinstance(img, torch.Tensor) else torch.from_numpy(img)).to('cpu')\n",
    "        img = img.permute(0,3,1,2)\n",
    "        img = img.float()  # uint8 to fp16/32\n",
    "        h, w, resize_value = Wrapper.get_lengths(img)\n",
    "        padding = torch.zeros((1, 3, resize_value, resize_value))\n",
    "        padding[:, :, :h, :w] = img\n",
    "        padding /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        return padding\n",
    "    \n",
    "    @staticmethod\n",
    "    def xywh2xyxy(x):\n",
    "        y = x.clone()\n",
    "        y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "        y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "        y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "        y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def _non_max_suppression(pred, orig_img, conf_threshold=0.5, iou_threshold=0.4, max_det=300):\n",
    "        pred.squeeze_()\n",
    "        boxes, scores, cls = pred[:4, :].T, pred[4:, :].amax(0), pred[4:, :].argmax(0).to(torch.int)\n",
    "        keep = scores.argsort(0, descending=True)[:max_det]\n",
    "        boxes, scores, cls = boxes[keep], scores[keep], cls[keep]\n",
    "        boxes = Wrapper.xywh2xyxy(boxes)\n",
    "        candidate_idx = torch.arange(0, scores.shape[0])\n",
    "        candidate_idx = candidate_idx[scores > conf_threshold]\n",
    "\n",
    "        boxes, scores, cls = boxes[candidate_idx], scores[candidate_idx], cls[candidate_idx]\n",
    "        final_idx = torchvision.ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
    "\n",
    "        boxes = boxes[final_idx]\n",
    "        scores = scores[final_idx]\n",
    "        cls = cls[final_idx]\n",
    "\n",
    "        boxes[:, [0,2]] = boxes[:, [0,2]].clamp(min=0, max=orig_img.size(2)) # width for x \n",
    "        boxes[:, [1,3]] = boxes[:, [1,3]].clamp(min=0, max=orig_img.size(1)) # height for y\n",
    "                \n",
    "        return torch.cat([boxes, scores.unsqueeze(1), cls.unsqueeze(1)], dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def postprocess(pred, orig_img):\n",
    "        result = Wrapper._non_max_suppression(pred, orig_img)\n",
    "        return result\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        orig_img = imgs.clone()\n",
    "        imgs = Wrapper.preprocess(imgs)\n",
    "        preds = self.model(imgs)\n",
    "        result = Wrapper.postprocess(preds[0], orig_img)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "image = torch.randint(0, 255, (1, 1080, 810,3), dtype=torch.uint8)\n",
    "yolo = YOLO(\"yolov8m.pt\", task='detect')\n",
    "model = yolo.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped = Wrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53065824508667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "result = wrapped(image)\n",
    "\n",
    "print(time() - start)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22863/1933337009.py:12: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  longest_side = torch.max(torch.tensor([h, w], dtype=torch.short).detach())\n",
      "/tmp/ipykernel_22863/1933337009.py:14: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return h, w, resize_value.int().item()\n",
      "/home/emre/workspaces/repositories/YoloV8/.venv/lib/python3.10/site-packages/ultralytics/nn/modules.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif self.dynamic or self.shape != shape:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dynamic = {}\n",
    "dynamic['image'] = {1 : 'height', 2 : 'width'} # Input shape: (1, H, W, 3)\n",
    "dynamic['output'] = {0 : 'num_boxes'} # Output shape: (N, 6)\n",
    "\n",
    "torch.onnx.export(\n",
    "    wrapped, \n",
    "    image, \n",
    "    'wrapped_model.onnx',\n",
    "    input_names=['image'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes=dynamic if dynamic else None,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import draw_bounding_boxes, save_image\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ONNXSession:\n",
    "    def __init__(self, model : str, data : str) -> None:\n",
    "        self.model = model\n",
    "        self.data = Path(data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_coco_labels_for_boxes(result):\n",
    "        indices = list(result[0][:, 5].astype('int'))\n",
    "        labels = []\n",
    "        for k in indices:\n",
    "            labels.append(yolo.names[k])\n",
    "        return labels\n",
    "    \n",
    "    def run(self, num_examples : Optional[int] = None):\n",
    "        if self.data.is_file():\n",
    "            paths = [self.data]\n",
    "        elif self.data.is_dir():\n",
    "            paths = sorted([path for path in self.data.iterdir()])\n",
    "\n",
    "        for path in tqdm(paths[:num_examples]):\n",
    "            img = read_image(str(path), ImageReadMode.RGB)\n",
    "            img_copy = img.clone()\n",
    "            img = img.permute(1,2,0).unsqueeze(0)\n",
    "\n",
    "            sess = ort.InferenceSession('wrapped_model.onnx')\n",
    "            inputs = sess.get_inputs()\n",
    "\n",
    "            result = sess.run(\n",
    "                None,\n",
    "                {inputs[0].name : img.numpy()}\n",
    "            )\n",
    "            # Create directory for results\n",
    "            results_dir = Path('results/')\n",
    "            if not results_dir.exists():\n",
    "                results_dir.mkdir(parents=True, exist_ok=True)\n",
    "            # Extract labels from class indices\n",
    "            labels = ONNXSession._get_coco_labels_for_boxes(result)\n",
    "            # Slice for boxes\n",
    "            boxes = torch.from_numpy(result[0][:, :4])\n",
    "            if boxes.numel() > 0:\n",
    "                img_copy = draw_bounding_boxes(img_copy, boxes, width=4, labels=labels)\n",
    "            save_image(img_copy / 255.0, f'{results_dir}/{path.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [03:30<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "sess = ONNXSession('wrapped_model.onnx', 'data/coco128/images/train2017/')\n",
    "sess.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39d91cea05d212b12221b5d2207b03cc88c9ad72bfc0ff987624f799986c9e15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
